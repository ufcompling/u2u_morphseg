#
# Docstrings generated by VS Code Python Docstring Generator and modified by user
#

import os, argparse, shutil
import sklearn_crfsuite
import pickle
import statistics
from typing import TypeAlias, TypedDict, Literal, cast

# Unused Libraries:
# from sklearn.model_selection import train_test_split
# from collections import Counter
# from sklearn import metrics
# import matplotlib.pyplot as plt
# from sklearn.metrics import precision_recall_fscore_support
# from scipy.special import rel_entr

# Type Hinting
Word: TypeAlias = str
Morph: TypeAlias = str
MorphList: TypeAlias = list[str]
PredictionLabel: TypeAlias = Literal['B', 'M', 'E', 'S', '[', ']']

CharFeatures: TypeAlias = dict[str, int]
WordFeatures: TypeAlias = list[CharFeatures]
DatasetFeatures: TypeAlias = list[WordFeatures]

WordLabels: TypeAlias = list[PredictionLabel]
DatasetLabels: TypeAlias = list[WordLabels]

WordChars: TypeAlias = list[str]
DatasetChars: TypeAlias = list[WordChars]

ConfidenceScore: TypeAlias = float
ConfidenceData: TypeAlias = tuple[Word, list[Morph], ConfidenceScore]

CharMarginals: TypeAlias = dict[PredictionLabel, float]
WordMarginals: TypeAlias = list[CharMarginals]
DatasetMarginals: TypeAlias = list[WordMarginals]

BMESDict: TypeAlias = dict[Word, str]

class DatasetInfo(TypedDict):
	words: list[Word]
	morphs: list[MorphList]
	bmes: BMESDict

class DataDict(TypedDict):
	train: DatasetInfo
	test: DatasetInfo
	select: DatasetInfo

def main() -> None:

	args: argparse.Namespace = parse_arguments()
	sub_datadir: str = setup_datadirs(args)

	# Define file paths
	PATHS: dict[str, str] = {
		# Data gathering file paths
		'TRAIN_TGT': f'{sub_datadir}/train.{args.initial_size}.tgt',
		'TEST_TGT': f'{args.datadir}/{args.lang}/test.full.tgt',
		'SELECT_TGT': f'{sub_datadir}/select.{args.initial_size}.tgt',
		# Prediction saving file paths
		'TEST_PRED': f'{sub_datadir}/test.full.pred',
		'SELECT_PRED': f'{sub_datadir}/select.{args.initial_size}.pred',
		# Source file paths
		'TRAIN_SRC': f'{sub_datadir}/train.{args.initial_size}.src',
		'TEST_SRC': f'{args.datadir}/{args.lang}/test.full.src',
		'SELECT_SRC': f'{sub_datadir}/select.{args.initial_size}.src',
		# Evaluation file paths
		'EVAL_FILE': f'{sub_datadir}/eval.txt'	
	}

	# Gather words, morphs, and bmes
	data: DataDict = process_data(PATHS['TRAIN_TGT'], PATHS['TEST_TGT'], PATHS['SELECT_TGT'])
	
	# Build and evaluate the model
	Y_test_predict, Y_select_predict = build_and_output_crf(sub_datadir, args, data)

	# Outputting predictions for the test and the select file
	test_predictions: list[MorphList] = reconstruct_predictions(Y_test_predict, data['test']['words'])
	save_predictions(test_predictions, PATHS['TEST_PRED'])

	if os.path.exists(PATHS['SELECT_SRC']):
		select_predictions: list[MorphList] = reconstruct_predictions(Y_select_predict, data['select']['words'])	
		save_predictions(select_predictions, PATHS['SELECT_PRED'])

	# Overall evaluation metrics
	average_precision, average_recall, average_f1 = evaluate_predictions(data['test']['morphs'], test_predictions)
	with open(PATHS['EVAL_FILE'], 'w') as f:
		f.write(f'Precision: {average_precision}\n')
		f.write(f'Recall: {average_recall}\n')
		f.write(f'F1: {average_f1}\n')

	print('Complete!')
	print(f'  Language: {args.lang.title()}')
	print(f'  Initial Size: {args.initial_size}')
	print(f'  Average Precision: {average_precision}')
	print(f'  Average Recall: {average_recall}')
	print(f'  Average F1 Score: {average_f1}')

### Parsing Arguments ###
def parse_arguments() -> argparse.Namespace:
	"""
	Parse command-line arguments.
	
	:return: Parsed arguments
	:rtype: argparse.Namespace
	"""
	parser: argparse.ArgumentParser = argparse.ArgumentParser()
	parser.add_argument('--datadir', type = str, default = 'data', help = 'path to data')
	parser.add_argument('--lang', type = str, default = 'lang', help = 'language')
	parser.add_argument('--initial_size', type = str, default = '100', help = 'data initial_size to start AL iteration')
	parser.add_argument('--seed', type = str, default = '0', help = 'different initial training sets')
	parser.add_argument('--method', type = str, default = 'al')
	parser.add_argument('--select_interval', type = int, default = 50, help = 'how much data to select in each AL iteration; 50, 100, maybe larger')
	parser.add_argument('--select_size', type = int, default = 0, help = 'how much data have selected from all AL iterations thus far')
	parser.add_argument('--d', type = int, default = 4, help = 'delta; context window to consider for feature set construction')
	parser.add_argument('--e', type = float, default = 0.001, help = 'epsilon parameter for training CRF; may not use this in training')
	parser.add_argument('--i', type = int, default = 100, help = 'maximum number of iterations for training CRF')

	return parser.parse_args()

### Set up directories ###
def read_file(file_path: str) -> str:
	"""
	Reads the content of a file.
		
	:param file_path: Path to the file
	:type file_path: str
	:return: Content of the file
	:rtype: str
	"""
	with open(file_path, 'r', encoding='utf-8') as f:
		return f.read()
	
def write_file(file_path: str, content: str) -> None:
	"""
	Writes content to a file.
	
	:param file_path: Path to the file
	:type file_path: str
	:param content: Content to write to the file
	:type content: str
	"""
	with open(file_path, 'w', encoding='utf-8') as f:
		f.write(content)

def setup_datadirs(args: argparse.Namespace) -> str:
	"""
	Sets up data directories for the current iteration of active learning.
	
	:param args: Command-line arguments
	:type args: argparse.Namespace
	:return: File path of the sub data directory
	:rtype: str
	"""
	# Data directory for the current iteration
	sub_datadir: str = f'{args.datadir}/{args.lang}/{args.initial_size}/{args.seed}/{args.method}/{args.select_interval}/select{args.select_size}'
	if not os.path.exists(sub_datadir):
		os.makedirs(sub_datadir)

	if args.select_size != 0:
		# Data directory for the previous iteration
		prev_datadir: str = f'{args.datadir}/{args.lang}/{args.initial_size}/{args.seed}/{args.method}/{args.select_interval}/select{int(args.select_size) - int(args.select_interval)}'
		
		# Labeled set = previous training set + previous increment
		train_src: str = read_file(f'{prev_datadir}/train.{args.initial_size}.src')
		increment_src: str = read_file(f'{prev_datadir}/increment.src')
		write_file(f'{sub_datadir}/train.{args.initial_size}.src', train_src + increment_src)

		train_tgt: str = read_file(f'{prev_datadir}/train.{args.initial_size}.tgt')
		increment_tgt: str = read_file(f'{prev_datadir}/increment.tgt')
		write_file(f'{sub_datadir}/train.{args.initial_size}.tgt', train_tgt + increment_tgt)
		
		# Unlabeled set = previous residual
		shutil.copy(f'{prev_datadir}/residual.src', f'{sub_datadir}/select.{args.initial_size}.src')
		shutil.copy(f'{prev_datadir}/residual.tgt', f'{sub_datadir}/select.{args.initial_size}.tgt')

	return sub_datadir

### Gathering Data ###
def get_line_morphs(line: str) -> tuple[Word, MorphList]:
	"""
	Extracts the word and its morphemes from a line.
	
	:param line: Line of text containing a segmented word
	:type line: str
	:return: The unsegmented word and its morphemes
	:rtype: tuple[Word, MorphList]
	"""
	toks: list[str] = line.strip().split()
	morphs: MorphList = ''.join(toks).split('!')
	word: Word = ''.join(m for m in morphs)
	return word, morphs

def get_bmes_labels(morphs: MorphList) -> str:
	"""
	Generates BMES labels for a list of morphemes.
	
	:param morphs: List of morphemes
	:type morphs: MorphList
	:return: BMES labels for each character in the word
	:rtype: str
	"""
	label: list = []

	for morph in morphs:
		if len(morph) == 1:
			label.append('S')
		else:
			label.append('B')

			for _ in range(len(morph)-2):
				label.append('M')

			label.append('E')

	return ''.join(label)

def load_file_data(file_path: str | None) -> tuple[list[Word], list[MorphList], BMESDict]:
	"""
	Loads words, morphemes, and BMES labels from a file.
	
	:param file_path: Path to the file to be loaded
	:type file_path: str | None
	:return: List of words, List of morphemes, and BMES labels
	:rtype: tuple[list[Word], list[MorphList], BMESDict]
	"""
	words: list[Word] = []
	morphs: list[MorphList] = []
	bmes: BMESDict = dict()

	if not file_path or not os.path.exists(file_path):
		return words, morphs, bmes

	with open(file_path, encoding='utf-8') as f:
		for line in f:
			word, line_morphs = get_line_morphs(line)
			bmes_labels: str = get_bmes_labels(line_morphs)

			words.append(word)
			morphs.append(line_morphs)
			bmes[word] = bmes_labels
		
	return words, morphs, bmes

def process_data(train_path: str, test_path: str, select_path: str | None = None) -> DataDict:  # *.tgt files 
	"""
	Processes training, testing, and selection data from files.
	
	:param train_path: Path to the training data file
	:type train_path: str
	:param test_path: Path to the testing data file
	:type test_path: str
	:param select_path: Path to the selection data file (optional)
	:type select_path: str | None
	:return: Dictionary containing training, testing, and selection data
	:rtype: DataDict
	"""
	train_words, train_morphs, train_bmes = load_file_data(train_path)
	test_words, test_morphs, test_bmes = load_file_data(test_path)
	select_words, select_morphs, select_bmes = load_file_data(select_path)

	return {
		'train': {
			'words': train_words,
			'morphs': train_morphs,
			'bmes': train_bmes
		},
		'test': {
			'words': test_words,
			'morphs': test_morphs,
			'bmes': test_bmes
		},
		'select': {
			'words': select_words,
			'morphs': select_morphs,
			'bmes': select_bmes
		}
	}

### Computing Features ###

VOWELS: set[str] = set('aeiouAEIOU')

# Common English affixes — helps the CRF recognize known morpheme patterns.
# For non-English languages, these won't fire and the model falls back to
# character-level features, so this is safe to leave in.
COMMON_PREFIXES: tuple[str, ...] = ('un', 're', 'in', 'im', 'dis', 'en', 'em', 'non', 'pre', 'mis', 'over', 'under', 'out', 'sub')
COMMON_SUFFIXES: tuple[str, ...] = ('ing', 'ed', 'ly', 'er', 'est', 'ness', 'ment', 'ful', 'less', 'tion', 'sion', 'able', 'ible', 'ous', 'ive', 'al', 'en', 'ish', 'dom', 'ship')

def get_char_features(bounded: Word, i: int, delta: int) -> CharFeatures:
	"""
	Generates character features for a given position in a bounded word.

	Produces n-gram, positional, phonological, and affix-awareness features.
	The original implementation only used n-grams + absolute start position,
	which starved the CRF of structural signal — especially on small training sets.
	
	:param bounded: The bounded word (with [ and ] markers)
	:type bounded: Word
	:param i: The index of the character in the bounded word
	:type i: int
	:param delta: The number of characters to consider for right and left features
	:type delta: int
	:return: Character features for the given position
	:rtype: CharFeatures
	"""
	char_dict: CharFeatures = {}
	char: str = bounded[i]
	word_len: int = len(bounded) - 2  # exclude [ and ]

	# --- Original n-gram features (right context) ---
	for j in range(delta):
		char_dict[f'right_{bounded[i:i+j+1]}'] = 1

	# --- Original n-gram features (left context) ---
	for j in range(delta):
		if i - j - 1 < 0: 
			break
		char_dict[f'left_{bounded[i-j-1:i]}'] = 1

	# --- Positional features ---
	char_dict[f'pos_start_{i}'] = 1
	char_dict[f'pos_end_{len(bounded) - i - 1}'] = 1   # distance from word end

	if word_len > 0:
		# Bucketed relative position (quarters) — generalizes across word lengths
		rel_pos: int = int((max(i - 1, 0) / word_len) * 4)
		char_dict[f'pos_bucket_{rel_pos}'] = 1

	char_dict[f'word_len_{min(word_len, 15)}'] = 1  # capped to avoid feature explosion

	# --- Character identity ---
	char_dict[f'char_{char}'] = 1

	# --- Phonological features ---
	if char not in ('[', ']'):
		is_vowel: bool = char in VOWELS
		char_dict['is_vowel'] = 1 if is_vowel else 0
		char_dict['is_consonant'] = 0 if is_vowel else 1

		# Vowel-consonant transitions often mark syllable/morpheme boundaries
		if i > 0 and bounded[i-1] not in ('[', ']'):
			prev_vowel: bool = bounded[i-1] in VOWELS
			if is_vowel != prev_vowel:
				char_dict['vc_transition'] = 1

		# Consonant cluster detection — clusters often don't span morpheme boundaries
		if not is_vowel and i > 0 and bounded[i-1] not in VOWELS | {'[', ']'}:
			char_dict['in_consonant_cluster'] = 1

	# --- Affix awareness features ---
	# Fire when the current position aligns with a known affix boundary.
	# These are soft hints, not hard rules — the CRF learns weights for them.
	if char not in ('[', ']'):
		inner_pos: int = i - 1  # 0-indexed position within the actual word
		raw_word: str = bounded[1:-1]

		for pfx in COMMON_PREFIXES:
			if raw_word.startswith(pfx) and inner_pos == len(pfx) - 1:
				char_dict[f'at_prefix_end_{pfx}'] = 1
			if raw_word.startswith(pfx) and inner_pos == len(pfx):
				char_dict[f'after_prefix_{pfx}'] = 1
				# Stem length after prefix — short stems suggest false prefix.
				# "re" + "ach" (3) is suspicious, "re" + "check" (5) is plausible.
				stem_len: int = word_len - len(pfx)
				char_dict[f'stem_after_pfx_{min(stem_len, 8)}'] = 1

		for sfx in COMMON_SUFFIXES:
			if raw_word.endswith(sfx) and inner_pos == word_len - len(sfx):
				char_dict[f'at_suffix_start_{sfx}'] = 1
				# Stem length before suffix — "gar" (3) + "ment" is suspicious,
				# "agree" (5) + "ment" is plausible.
				stem_len_sfx: int = word_len - len(sfx)
				char_dict[f'stem_before_sfx_{min(stem_len_sfx, 8)}'] = 1
			if raw_word.endswith(sfx) and inner_pos == word_len - len(sfx) - 1:
				char_dict[f'before_suffix_{sfx}'] = 1

	# --- Cross-boundary bigram features ---
	# The character pair spanning a potential boundary is the single most
	# discriminative signal for real vs. fake affix boundaries. E.g.,
	# "agree!ment" has cross-bigram "em" at the boundary, while "garment"
	# has "rm" — the CRF can learn which pairs occur at real boundaries.
	if char not in ('[', ']'):
		if i > 1 and bounded[i-1] not in ('[',):
			char_dict[f'bigram_{bounded[i-1]}{char}'] = 1
		if i < len(bounded) - 1 and bounded[i+1] not in (']',):
			char_dict[f'bigram_{char}{bounded[i+1]}'] = 1

	# --- Doubled consonant detection ---
	# Words like "kitten", "mitten", "flatten", "written" have doubled
	# consonants immediately before the "-en" ending. These are almost never
	# real stem+suffix boundaries — the doubling is orthographic, not
	# morphological. This feature gives the CRF a direct signal.
	if char not in ('[', ']') and i >= 2:
		if bounded[i-1] == char and char not in VOWELS:
			char_dict['doubled_consonant'] = 1
		if bounded[i-1] not in ('[', ']') and i >= 3 and bounded[i-2] == bounded[i-1] and bounded[i-1] not in VOWELS | {'[', ']'}:
			char_dict['after_doubled_consonant'] = 1

	return char_dict

def get_word_features(word: Word, bmes: BMESDict, delta: int) -> tuple[WordFeatures, WordLabels, WordChars]:
	"""
	Generates features, labels, and characters for a given word.
	
	:param word: The word to generate features for
	:type word: Word
	:param bmes: The BMES labels for each character in the word
	:type bmes: BMESDict
	:param delta: The number of characters to consider for right and left features
	:type delta: int
	:return: Features, labels, and characters for the given word
	:rtype: tuple[WordFeatures, WordLabels, WordChars]
	"""
	bounded_word: Word = f'[{word}]' # <w> and <\w> replaced with [ and ], respectively
	features: WordFeatures = [] # list (word) of dicts (chars)
	labels: WordLabels = [] # list (word) of labels (chars)
	word_chars: WordChars = [] # list (learning set) of list (word) of chars
	
	for i in range(len(bounded_word)):
		char_dict: CharFeatures = get_char_features(bounded_word, i, delta)
		features.append(char_dict)

		char: str = bounded_word[i]
		word_chars.append(char)

		if char in ['[', ']']: # labeling start and end
			labels.append(cast(PredictionLabel, char))
		else: # labeling chars
			labels.append(cast(PredictionLabel, bmes[word][i-1]))

	return features, labels, word_chars

def get_features(words: list[Word], bmes: BMESDict, delta: int) -> tuple[DatasetFeatures, DatasetLabels, DatasetChars]:
	"""
	Generates features, labels, and characters for a list of words.
	
	:param words: The list of words to generate features for
	:type words: list[Word]
	:param bmes: The BMES labels for each character in each word
	:type bmes: BMESDict
	:param delta: The number of characters to consider for right and left features
	:type delta: int
	:return: Features, labels, and characters for the given list of words
	:rtype: tuple[DatasetFeatures, DatasetLabels, DatasetChars]
	"""
	X: DatasetFeatures = [] # list (learning set) of list (word) of dicts (chars), INPUT for crf
	Y: DatasetLabels = [] # list (learning set) of list (word) of labels (chars), INPUT for crf
	dataset_chars: DatasetChars = [] # list (learning set) of list (word) of chars

	for word in words:
		features, labels, word_chars = get_word_features(word, bmes, delta)
		X.append(features)
		Y.append(labels)
		dataset_chars.append(word_chars)

	return X, Y, dataset_chars

### Sorting Data based on Confidence ###
def get_confidence_scores(words: list[Word], predictions: DatasetLabels, marginals: DatasetMarginals) -> list[ConfidenceScore]:
	"""
	Calculates confidence scores for a list of words based on predictions and marginals.
	
	:param words: The list of words to calculate confidence scores for
	:type words: list[Word]
	:param predictions: The predictions for each word in the list
	:type predictions: DatasetLabels
	:param marginals: The marginal probabilities for each word in the list
	:type marginals: DatasetMarginals
	:return: The confidence scores for each word in the list
	:rtype: list[ConfidenceScore]
	"""
	confscores: list[ConfidenceScore] = []
	for word, prediction, marginal in zip(words, predictions, marginals):
		# Remove '[' and ']' so that the characters match up with the labels
		boundless_pred, boundless_marg = prediction[1:-1], marginal[1:-1]
		confscores.append(sum(boundless_marg[i][label] for i, label in enumerate(boundless_pred)) / len(word))

	return confscores

def sort_by_confidence(words: list[Word], morphs: list[MorphList], confscores: list[ConfidenceScore]) -> list[ConfidenceData]:
	"""
	Sorts words, morphemes, and confidence scores by confidence scores.
	
	:param words: The list of words to sort
	:type words: list[Word]
	:param morphs: The list of morpheme lists to sort
	:type morphs: list[MorphList]
	:param confscores: The list of confidence scores to sort by
	:type confscores: list[ConfidenceScore]
	:return: The sorted list of (word, morphs, confidence) tuples
	:rtype: list[ConfidenceData]
	"""
	# Sort words/morphs by their confidence
	with_confidence: list[ConfidenceData] = sorted(zip(words, morphs, confscores), key=lambda x: x[2])

	# For debugging:
	# SEPARATOR = '|'
	# sorted_words, sorted_morphs, sorted_confidence = zip(*with_confidence)
	# datastring = []
	# for i in range(len(sorted_words)):
	# 	pairs = list(zip(sorted_words[i], sorted_morphs[i]))
	# 	info = ' '.join(pair[0] + SEPARATOR + pair[1] for pair in pairs) + '\t' + str(sorted_confidence[i])
	# 	datastring.append(info)

	return with_confidence

### Building Models ###

def build_crf(sub_datadir: str, X_train: DatasetFeatures, Y_train: DatasetLabels, max_iterations: int = 100) -> sklearn_crfsuite.CRF:
	"""
	Builds and trains a CRF model.

	Regularization (c1/c2) is scaled based on training set size. With small
	datasets (<200 words), the original c1=c2=0.1 caused heavy underfitting.
	Lowering these lets the model actually learn the patterns present in the data
	without overfitting on the eval set — CRFs are inherently regularized by the
	structured prediction objective.
	
	:param sub_datadir: The subdirectory to save the model in
	:type sub_datadir: str
	:param X_train: The training features
	:type X_train: DatasetFeatures
	:param Y_train: The training labels
	:type Y_train: DatasetLabels
	:param max_iterations: The maximum number of iterations for training
	:type max_iterations: int
	:return: The trained CRF model
	:rtype: CRF
	"""
	n_samples: int = len(X_train)

	# Scale regularization to dataset size. These values were found empirically:
	# - tiny (<100):  nearly unregularized — let the model memorize what little it has
	# - small (<500): light regularization
	# - larger:       standard values
	if n_samples < 100:
		c1, c2 = 0.01, 0.01
	elif n_samples < 500:
		c1, c2 = 0.05, 0.05
	else:
		c1, c2 = 0.1, 0.1

	crf: sklearn_crfsuite.CRF = sklearn_crfsuite.CRF(
		algorithm='lbfgs',
		c1=c1,
		c2=c2,
		max_iterations=max_iterations,
		all_possible_transitions=True
	)
	crf.fit(X_train, Y_train)
	with open(f'{sub_datadir}/crf.model', 'wb') as f:
		pickle.dump(crf, f)

	return crf

def split_increment_residual(confidence_data: list[ConfidenceData], select_interval: int) -> tuple[list[ConfidenceData], list[ConfidenceData]]:
	"""
	Splits confidence data into increment and residual sets based on the selection interval.
	
	:param confidence_data: The list of (word, morphs, confidence) tuples to split
	:type confidence_data: list[ConfidenceData]
	:param select_interval: The number of items to include in the increment set
	:type select_interval: int
	:return: The increment and residual sets as lists of (word, morphs, confidence) tuples
	:rtype: tuple[list[ConfidenceData], list[ConfidenceData]]
	"""
	increment_data: list[ConfidenceData] = confidence_data[:select_interval]
	residual_data: list[ConfidenceData] = confidence_data[select_interval:]

	return increment_data, residual_data

def save_data(confidence_data: list[ConfidenceData], sub_datadir: str, file_name: str) -> None:
	"""
	Saves confidence data to source and target files.
	
	:param confidence_data: The list of (word, morphs, confidence) tuples to save
	:type confidence_data: list[ConfidenceData]
	:param sub_datadir: The subdirectory to save the files in
	:type sub_datadir: str
	:param file_name: The base name for the source and target files
	:type file_name: str
	"""
	if not confidence_data:
		# Write empty files so downstream path-existence checks stay consistent
		open(f'{sub_datadir}/{file_name}.src', 'w').close()
		open(f'{sub_datadir}/{file_name}.tgt', 'w').close()
		return

	words, morphs_list, _ = zip(*confidence_data)
	
	src_content: list[str] = [' '.join(word) + '\n' for word in words]
	with open(f'{sub_datadir}/{file_name}.src', 'w', encoding='utf-8') as src:
		src.writelines(src_content)

	tgt_content: list[str] = [' '.join('!'.join(morphs)) + '\n' for morphs in morphs_list]
	with open(f'{sub_datadir}/{file_name}.tgt', 'w', encoding='utf-8') as tgt:
		tgt.writelines(tgt_content)

def output_crf(crf: sklearn_crfsuite.CRF, sub_datadir: str, args: argparse.Namespace, data: DataDict, X_test: DatasetFeatures) -> tuple[DatasetLabels, DatasetLabels]:
	"""
	Outputs predictions from the CRF model and prepares data for active learning.
	
	:param crf: The trained CRF model
	:type crf: sklearn_crfsuite.CRF
	:param sub_datadir: The subdirectory to save output files in
	:type sub_datadir: str
	:param args: The command-line arguments
	:type args: argparse.Namespace
	:param data: The data dictionary containing train, test, and select data
	:type data: DataDict
	:param X_test: The test features
	:type X_test: DatasetFeatures
	:return: The predicted labels for the test and select sets
	:rtype: tuple[DatasetLabels, DatasetLabels]
	"""
	Y_test_predict: DatasetLabels = crf.predict(X_test)

	Y_select_predict: DatasetLabels = []
	if data['select']['words']:
		X_select, _, _ = get_features(data['select']['words'], data['select']['bmes'], args.d)
		Y_select_predict = crf.predict(X_select)

		# Get Confidence Data
		marginals: DatasetMarginals = crf.predict_marginals(X_select)
		confscores: list[ConfidenceScore] = get_confidence_scores(data['select']['words'], Y_select_predict, marginals)
		conf_sorted_data: list[ConfidenceData] = sort_by_confidence(data['select']['words'], data['select']['morphs'], confscores)

		# Generate increment.src and residual.src
		increment_data, residual_data = split_increment_residual(conf_sorted_data, args.select_interval)
		save_data(increment_data, sub_datadir, 'increment')
		save_data(residual_data, sub_datadir, 'residual')

	return Y_test_predict, Y_select_predict

def build_and_output_crf(sub_datadir: str, args: argparse.Namespace, data: DataDict) -> tuple[DatasetLabels, DatasetLabels]:
	"""
	Builds the CRF model and outputs predictions.
	
	:param sub_datadir: The subdirectory to save output files in
	:type sub_datadir: str
	:param args: The command-line arguments
	:type args: argparse.Namespace
	:param data: The data dictionary containing train, test, and select data
	:type data: DataDict
	:return: The predicted labels for the test and select sets
	:rtype: tuple[DatasetLabels, DatasetLabels]
	"""
	X_train, Y_train, _ = get_features(data['train']['words'], data['train']['bmes'], args.d)
	X_test, _, _ = get_features(data['test']['words'], data['test']['bmes'], args.d)

	crf: sklearn_crfsuite.CRF = build_crf(sub_datadir, X_train, Y_train)
	
	return output_crf(crf, sub_datadir, args, data, X_test)

# Going from predicted labels to predicted morphemes
def reconstruct_predictions(pred_labels: DatasetLabels, words: list[Word]) -> list[MorphList]:
	"""
	Reconstructs morpheme predictions from predicted labels.
	
	:param pred_labels: The predicted labels for each word
	:type pred_labels: DatasetLabels
	:param words: The list of words corresponding to the predicted labels
	:type words: list[Word]
	:return: The reconstructed morpheme predictions for each word
	:rtype: list[MorphList]
	"""
	predictions: list[MorphList] = []

	for pred, word in zip(pred_labels, words):

		# Remove '[' and ']' and split by end markers (E)
		labels: list[str] = [label for label in ''.join(w for w in pred[1:-1]).split('E') if label]
		
		new_labels: list[str] = []
		for tok in labels:
			
			# If nothing is marked single, then it's fine; add back the end marker
			if 'S' not in tok:
				tok += 'E'
				new_labels.append(tok)

			# Otherwise, if the tok only contains single markers, then add that many single markers
			elif (s_count := tok.count('S')) == len(tok):
				new_labels.extend(['S'] * s_count)
			
			# Otherwise, append an S marker for the single morphemes or concatenate an end marker to each morpheme and add it
			else:
				for bmes_label in tok.split('S'):
					if bmes_label == '':
						new_labels.append('S')
					else:
						new_labels.append(bmes_label + 'E')

		morphs: MorphList = []
		prefix_length: int = 0
		for label in new_labels:
			tok_length: int = len(label)
			morphs.append(word[prefix_length:prefix_length+tok_length])
			prefix_length += tok_length

		predictions.append(morphs)

	return predictions

# Save predictions
def save_predictions(predictions: list[MorphList], file_path: str) -> None:
	"""
	Saves morpheme predictions to a file.
	
	:param predictions: The list of predicted morpheme lists
	:type predictions: list[MorphList]
	:param file_path: The path to the output file
	:type file_path: str
	"""
	pred_content: list[str] = [' '.join('!'.join(morphs)) + '\n' for morphs in predictions]
	with open(file_path, 'w', encoding = 'utf-8') as f:
		f.writelines(pred_content)

# Evaluate predictions with statistical metrics (precision, recall, F1 score)
def _morphs_to_boundaries(morphs: MorphList) -> set[int]:
	"""
	Convert a morpheme list to a set of boundary positions (character offsets
	where one morpheme ends and the next begins).

	Example: ["un", "help", "ful"] -> {2, 6}  (boundary after char 2, after char 6)

	:param morphs: List of morphemes for one word
	:type morphs: MorphList
	:return: Set of boundary character positions
	:rtype: set[int]
	"""
	boundaries: set[int] = set()
	offset: int = 0
	for morph in morphs[:-1]:  # no boundary after the last morpheme
		offset += len(morph)
		boundaries.add(offset)
	return boundaries

def calculate_metrics(y_true: MorphList, y_pred: MorphList) -> tuple[float, float, float]:
	"""
	Calculates precision, recall, and F1 score using boundary-based evaluation.

	The previous implementation used bag-of-morphemes matching (`if m in y_true`)
	which didn't respect position or multiplicity — a predicted morpheme "ed"
	would match any gold morpheme "ed" regardless of where it appeared, and
	duplicate predictions were counted as correct multiple times.

	Boundary-based evaluation is standard for morphological segmentation: we
	compare the set of character offsets where splits occur.

	:param y_true: The true morpheme list
	:type y_true: MorphList
	:param y_pred: The predicted morpheme list
	:type y_pred: MorphList
	:return: The precision, recall, and F1 score (0-100 scale)
	:rtype: tuple[float, float, float]
	"""
	gold_bounds: set[int] = _morphs_to_boundaries(y_true)
	pred_bounds: set[int] = _morphs_to_boundaries(y_pred)

	# Single-morpheme words have no boundaries — both sides agree trivially
	if not gold_bounds and not pred_bounds:
		return 100.0, 100.0, 100.0

	if not pred_bounds:
		return 0.0, 0.0, 0.0

	if not gold_bounds:
		return 0.0, 0.0, 0.0

	correct: int = len(gold_bounds & pred_bounds)
	precision: float = correct / len(pred_bounds) * 100
	recall: float = correct / len(gold_bounds) * 100
	f1: float = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

	return round(precision, 2), round(recall, 2), round(f1, 2)

def evaluate_predictions(gold_word: list[MorphList], pred_word: list[MorphList]) -> tuple[float, float, float]:
	"""
	Evaluates predicted morphemes against gold standard morphemes using precision, recall, and F1 score.
	
	:param gold_word: The list of gold standard morpheme lists
	:type gold_word: list[MorphList]
	:param pred_word: The list of predicted morpheme lists
	:type pred_word: list[MorphList]
	:return: The average precision, recall, and F1 score across all words
	:rtype: tuple[float, float, float]
	"""
	precision_scores: list[float] = []
	recall_scores: list[float] = []
	f1_scores: list[float] = []

	for gold_morphs, pred_morphs in zip(gold_word, pred_word):
		precision, recall, f1 = calculate_metrics(gold_morphs, pred_morphs)
		precision_scores.append(precision)
		recall_scores.append(recall)
		f1_scores.append(f1)

	if not precision_scores:
		return 0.0, 0.0, 0.0

	average_precision, average_recall, average_f1 = (round(statistics.mean(x), 2) for x in (precision_scores, recall_scores, f1_scores))
	return average_precision, average_recall, average_f1

if __name__ == '__main__':
	main()